<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Reliability and Validity</title>
    <meta charset="utf-8" />
    <meta name="author" content="ALEXANDER WEISS" />
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Reliability and Validity
## Research Methods and Statistics 2<br><br>
### ALEXANDER WEISS
### Department of Psychology<br>The University of Edinburgh
### AY 2020-2021

---






# Today's topics
+ Reliability
  + Measurement
  + True scores
  + Types of reliability
  
  
+ Validity
  + Relationship to reliability
  + Debates on the concept
  + Types of validity
  + What can it tell us?
  
---

# Acknowledgement
+ Slides *heavily* influenced by work of Bill Revelle and his [book on
  psychometrics](http://www.personality-project.org/r/book/)

---

# Measurement
+ The aim of measurement is to develop and use measures of constructs
  to test psychological theories
  
  
+ 'Classical test theory' describes data from any measure as a
  combination of
  + The signal of the construct, or the 'true score'
  + Noise or 'error', that is, measure of other, unintended things
  
`$$\text{Observed score} = \text{True score} + \text{Error}$$`


+ This should remind you of the factor analysis formula that you
  learned in the last lecture
  
---

# True score theory
+ If we assume of our test that:
	1. It measures some ability or trait
	2. In the world, there is a "true" value or score for this test
       for each individual


+ Then the reliability of the test is a measure of how well it
  reflects the true score
  
---
# Parallel tests
+ Charles Spearman was the first to note that, under certain
  assumptions, the correlations between two **parallel tests** of the
  same construct provided an estimate of reliability
  + Assumptions of **parallelism**
	  + Both tests have the same relationship to the true score
	  + Both tests have the same error variance
+ As the number of tests increases, we can make fewer assumptions
  + Tau equivalent reliability
	  + Tests have the same relationship to the true score
	  + Error variances, however, are allowed to differ
  + Congeneric reliability
	  + Only possible with four or more tests
	  + Each test is an imperfect measure of the construct
	  + Tests' relationship with the true score can differ; error
        variances for each test can differ
		
---
&lt;center&gt;
&lt;img src="figures/measurement_model.png" alt="measurement" width="500"/&gt;
&lt;/center&gt;


What paths (arrows) between observed test scores, `\(X\)`, and true score,
`\(T\)`, and/or error variance, `\(e\)`, need to be set equal?

What method of factor analysis is best-suited for this task?

Are tau-equivalence and parallel forms reliability special cases of
congeneric reliability?

---

# Where do parallel tests come from?
+ Previous, older definitions of "parallel tests" were somewhat
  abstract
  
  
+ Parallel tests can come from several sources
  + Time tests were administered
  + Raters
  + Items

---
# Alternate forms reliability
+ Correlation between two variants of a test
  + Same items in different order (randomization of stimuli)
  + Tests with similar, but not identical, content, e.g., tests with a
  fixed number of basic addition, subtraction, division, and
  multiplication problems
  + Ideally, alternate tests have equal means and variances


+ Assumption is that, if the tests are perfectly reliable, then they
  should correlate perfectly
  + They won't
  + To the extent that they don't, we have a measure of reliability
  
---

# Alternate forms reliability
+ When we have 4+ tests, we can use factor loadings to estimate each
  test's relation to the true score (the latent variable)
  + Also provides information on (congeneric) reliability
  + Hence why it's nice to have more than three items per factor
	
---

# Alternate forms reliability
+ Developing alternate forms is becoming much easier
  1. Write a large item bank
  2. Get many respondents
  3. Use item-response theory (IRT) to assess how "difficult" items are
  4. Create your tests with items matched on difficulty


+ This is what computerized adaptive testing is all about

---

# Test-retest reliability
+ Correlation between tests taken at 2+ points in time (think again
  about parallel, tau equivalent, and congeneric
    reliability)
	

+ Corner-stone of test assessment and appears in many test manuals,
  but poses some tricky questions
  + What's the appropriate time between when measures are taken?
  + How stable should the construct be if we are to consider it a
  trait?


+ **Remember**: Even if you have within-individual changes in mean
  scores, so long as rank ordering is consistent, correlations can
  stay high

---

# Split-half reliability
+ This measure of reliable indicates how internally consistent the
  test is
  1. Split test into a pair of equal subsets of `\(n\)` items
  2. Score the two subsets
  3. Correlate these scores
  
  
+ With an increasing number of items, the number of possible splits
  gets very large, the relationship is:

`$$\frac{n!}{2\left(\frac{n}{2}\right)!^{2}}$$`

---

+ You can write an R function to see or plot this


```r
poss_splits &lt;- function(n) {(factorial(n) / (2*(factorial(n/2))^2))}

poss_splits(4)
```

```
## [1] 3
```

```r
curve(poss_splits, from=4, to=20)
```

&lt;img src="dapr3_factor_analysis_4_files/figure-html/split_test_function-1.png" style="display: block; margin: auto;" /&gt;
---

# Cronbach's alpha
+ If we take the idea of correlating subsets of items to its logical conclusion
  + Split-half reliability is a special case of reliability among all
  test items
  + The best known estimate of this is formula 2 from Cronbach (1951)
  
`$$\alpha=\frac{n}{n-1}\left(1-\frac{\sum\limits_{i=1}^n V_{i}}{V_{t}}\right)$$`

`\(V_{t}\)` is the variance of test scores (total variance)

`\(V_{i}\)` is the variance of the `\(n\)` item scores after they've been
weighted (error variance)

---

# Cronbach's alpha
+ It does **not** indicate whether items measure one unidimensional construct
  + This is clear when one recognizes that Cronbach's alpha increases
  as you add items, so it can be high even if there's no underlying
  factor
  + Known as 'Spearman-Brown prophecy formula'
  
`$$\rho^{*}_{xx'}=\frac{n\rho_{xx'}}{1+(n-1)\rho_{xx'}}$$`

`\(\rho^{*}_{xx'}\)` is the predicted (prophesized) Cronbach's alpha

`\(\rho_{xx'}\)` is the original Cronbach's alpha

`\(n\)` ratio of number of new to old measures, so, e.g., if `\(n=2\)`, the
new test has twice as many items; can also be a fraction

---

# McDonald's omega
+ Any item may measure
  + A "general" factors that load on all items
  + A "group" or "specific" factor that loads on a subset of items


+ Given this, we can derive two internal consistency measures
  + Omega hierarchical `\((\omega_{h})\)`, the proportion of item variance
  that is general
  + Omega total `\((\omega_{t})\)`, the total proportion of reliable item variance


+ Estimating these values
  + Use `omega` function in the psych package
  + Use CFA and compute it manually
	
---

# Interrater reliability
+ Ask a set of judges to rate a set of targets
  + Get friends to rate the personality of a family member
  + Get zoo keepers to rate the subjective well-being of an animal
  

+ We can determine how consistent raters are by means of intraclass
  correlation coefficients
  + How reliable are their individual estimates
  + How reliable is the average estimate based on the judges' ratings

---

# Intraclass correlations
+ Splits variance of a set of ratings into multiple components
  + Variance between subjects (across targets)
  + Variance within subjects (across raters, same target)
  + Variance due to raters (across targets, same rater)


+ Depending on what we want to know and the design of our study, we
  can calculate intraclass correlations from these variance components
  
---

# Intraclass correlations

| ICC | Description                                                                                                                                         |
|:----|:----------------------------------------------------------------------------------------------------------------------------------------------------|
| 1,1 | Targets rated by a different set of randomly selected raters and reliability is based on one measurement                                            |
| 1,k | As (1,1), but with reliability calculated as the average of k raters’ measurements                                                                  |
| 2,1 | Each target measured by each rater. Raters are considered representative of larger pool of raters. Reliability calculated from a single measurement |
| 2,k | As (2,1), but with reliability calculated as the average of k raters’ measurements                                                                  |
| 3,1 | Each target measured by each rater. Raters only raters of interest. Reliability calculated from a single measurement                                |
| 3,k | As (3,1), but with reliability calculated as the average of k raters’ measurements                                                                  |
&lt;p align='right'&gt;Shrout and Fleiss (1979)&lt;/p&gt;

---

# Intraclass correlations
+ The `ICC` function from the psych package can be used, but it only
  works on balanced data in "wide" format, so it's sometimes not
  convenient

|          | Judge 1 | Judge 2 | Judge 3 | Judge 4 |
|---------:|--------:|--------:|--------:|--------:|
| Target 1 |       9 |       2 |       5 |       8 |
| Target 2 |       6 |       1 |       3 |       2 |
| Target 3 |       8 |       4 |       6 |       8 |
| Target 4 |       7 |       1 |       2 |       6 |
| Target 5 |      10 |       5 |       6 |       9 |
| Target 6 |       6 |       2 |       4 |       7 |

---

# Intraclass correlations
+ To deal with unbalanced data, start by getting it into "long" format

| judge_id | target_id | score |
|---------:|----------:|------:|
|        1 |         1 |     9 |
|        1 |         2 |     6 |
|        1 |         3 |     8 |
|        . |         . |     . |
|        . |         . |     . |
|        . |         . |     . |
|        4 |         4 |     6 |
|        4 |         5 |     9 |
|        4 |         6 |     7 |

---
# Intraclass correlations
+ Use R to obtain your variance components


+ Be sure that your Judge and Target variables are treated as factors!


```r
iccs &lt;- lm(score ~ as.factor(judge_id) + as.factor(target_id))
anova(iccs)
```

+ The mean square values from the second command are estimates of
  variance components

---

# Generalizability theory
+ Extension of classical test theory (Shavelson, 1989)
  + To what extent does a score generalize to the universe of possible...
  + More flexible, multiple definitions of 'true score' and 'error'

---

# Uses of reliability
+ Good to know how reliable a measure is
  + Implications for validity (will discuss it soon)
  + Also allows us to 'correct for attenuation'
  
`$$r^{*}_{xy}=\frac{r_{xy}}{\sqrt{\rho^{2}_{\theta x}\rho^{2}_{\theta y}}}$$`

`\(r^{*}_{xy}\)` is the correlation between `\(x\)` and `\(y\)` after correcting for attenuation

`\(r_{xy}\)` is the correlation before correcting for attenuation

`\(\rho^{2}_{\theta x}\)` is the reliability of `\(x\)`

`\(\rho^{2}_{\theta y}\)` is the reliability of `\(y\)`

---
# Reliability in R

| Reliability over | Estimate        | Functions in R |
|:-----------------|:----------------|:---------------|
| Forms            | Alternate forms | cor            |
| Time             | Test-retest     | cor            |
|                  |                 | rptR           |
| Split-half       | Random split    | splitHalf      |
|                  | Worst split     | splitHalf      |
|                  | Best split      | splitHalf      |
| Items            | General factor  | omega          |
|                  | Average         | alpha          |
| Raters           | All variants    | ICC            |

---

# Validity
&gt; Validity refers to the degree to which evidence and theory support
&gt; the interpretations of test scores for proposed uses of
&gt; tests. Validity is, therefore, the most fundamental consideration in
&gt; developing tests and evaluating tests. The process of validation
&gt; involves accumulating relevant evidence to provide a sound scientific
&gt; basis for the proposed score interpretations. It is the
&gt; interpretations of the test scores for the proposed uses that are
&gt; valuated, not the test itself.
&lt;p align='right'&gt;Standard for Educational and Psychological Testing&lt;/p&gt;

---

# Debates about the definition
*[W]hether a test really measures what it purports to measure* (Kelley,
1927)

*[H]ow well a test does the job it is employed to do. The same may be
used for ... different purposes and its validity may be high for
one, moderate for another and low for a third* (Cureton, 1951)

*Validity is "an integrated evaluative judgment of the degree to
which empirical evidence and theoretical rationales support the
adequacy and appropriateness of inferences and actions based on test
scores or other modes of assessment* (Messick, 1989)

*A test is valid for measuring an attribute if (a) the attribute
exists and (b) variations in the attribute causally produce
variation in the measurement outcomes* (Borsboon et al., 2004)

*[V]alidity means that the information yielded by a test is
appropriate, meaningful, and useful for decision making -- the purpose
of mental measurement* (Osterlind, 2010)

---

# Evidence for validity
+ Debates about how to define validity lead to questions about what
  constitutes evidence for validity
  

+ Sources of evidence align to what may be viewed as "classical"
  concepts reported in textbooks, studies, and test manuals

---

# Evidence related to content
+ Content validity
  + A test should contain only content relevant to the intended
  construct
  + It should measure what it was intended to measure
  
  
+ Face validity, i.e., for those taking the test, does the test
  "appear to" measure what it was designed to measure?

---

# Evidence related to the scale
+ Do the items measure a single "intended" construct?


+ Factor analysis only provides very limited information towards this


+ How else might we assess it?

---

# Relationships with other constructs
+ Construct validity, or convergent and discriminant validity
  (Cronbach &amp; Meehl, 1955)
  + **Convergent**: Measure should have high correlations with other
  measures of the same construct
  + **Discriminant**: Measure should have low correlations with
  measures of different constructs
  + **Nomological Net**
	  + Measure should have expected patterns (positive/negative)
	  correlations with different sets of constructs
	  + Also, some measures should vary depending on manipulations,
      e.g., a measure of "stress" should be higher among students who are
      told that a test is "high stakes" than among students told that
      a test is "low stakes"
---

# Relationships with other constructs
+ Consider relations in terms of temporal sequence
  + Concurrent validity: Correlations with contemporaneous measures
	  + Neuroticism and subjective well-being
	  + Extraversion and leadership
   + Predictive validity: Related to expected future outcomes
	 + IQ and health
	 + Agreeableness and future income
---

# Evidence related to response processes
+ Discussed in a paper by Karabenick et al. (2007)
  + Not commonly considered in validation studies
  + Is how people "process" the items belonging to a scale the way we
    think they ought to?
	+ Do tests of intelligence engage problem-solving behaviors?
	+ Do extraversion items lead people to reflect on related past
      behaviors?
	  
---

# Evidence related to consequences
+ Perhaps most controversial aspect of current validity discussions


+ Should potential consequences of test use be considered part of the
  evidence for test's validity?


+ Important questions for the use of tests
  + Is my measure systematically biased or fair for all groups of test
  takers?
  + Does bias have social ramifications?

---
# Example: Implicit association test
+ Reliability is okay, but not great


+ Weakly (if at all) predicts discriminatory behavior


+ Used to label people, decide who gets certain jobs, etc.
  
---

# Definition redux (Hughes, in press)
+ Validity has many meanings, some markedly different


+ When using the term, examine which evidence has been used to
  'determine validity'
  
  
+ Validation is on-going process concerning **accuracy** and
  **appropriateness** of a test

---

# Accuracy of the test
+ Content


+ Response processes


+ Structural (within and across groups)


+ Convergent and discriminant

---

# Appropriateness of the test
+ Predictive, concurrent, incremental


+ Know groups who test is designed for


+ Consequences (fairness, bias)


+ Feasibility (cost, length, etc.)


---

# Relationship between reliability and validity
+ Reliability: relation of true score with observed score


+ Validity: correlations with other measures play a key role


+ Logically, a score or measure cannot correlate with anything more
  than it correlates with itself, so reliability is the limit on
  validity

---

# Importance of test reliability and validity
+ Fundamental first step in measurement
  + If we cannot measure well variables of interest, then we cannot
  study them
  + Large, tricky problem in psychology; many variables/constructs are not directly accessible


+ Important for later research
  + Poor reliability and validity may lead to erroneous conclusions
  due to measurement problems
  + If we know reliability of test, we can sometimes make adjustments
  (correction for attenuation)
  + Not "glamorous" research, but can be interesting in and of itself

---

# Validity studies
+ Different forms
  + New measure given as part of a large battery
	+ Concurrent validity (convergent and discriminant) considered; factorial validity, too
	+ Existing samples may be followed up for predictive validity

---
# Where can you find this information?
+ Test manuals


+ Papers describing new tests and papers investigating exisitng
  measures in different groups, languages, contexts, etc.
  + *Assessment*
  + *Psychological Assessment*
  + *European Journal of Psychological Assessment*
  + *Organisational Research Methods*
  + Personality journals
  
  
+ Papers describing new ways to establish reliability, validity, etc.
  + *Methdology*
  + *Psychometrika*
  + *Journal of Educational Statistics*
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>

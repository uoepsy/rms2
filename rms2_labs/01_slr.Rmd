---
title: "Simple linear regression"
bibliography: references.bib
biblio-style: apalike
link-citations: yes
params: 
    SHOW_SOLS: TRUE
    TOGGLE: TRUE
---


```{r setup, include=FALSE}
source('assets/setup.R')

# knitr::opts_chunk$set(cache = TRUE)
set.seed(1)
```


This set of exercises gradually introduces simple linear regression. It starts by stating the research question of interest, performing an exploratory analysis of the data, fitting the model, and understanding the components of the fitted model.

For those of you who have previously taken a first course in data analysis or statistics, such as RMS1, this will be a refresher.


# Research question

Let’s imagine a study into income disparity for workers in a local authority. We might carry out interviews and find that there is a link between the level of education and an employee’s income. Those with more formal education seem to be better paid. 
Now we wouldn’t have time to interview everyone who works for the local authority so we would have to interview a sample, say 10%.

In this lab we will use the riverview data (see below) to examine whether education level is related to income among the employees working for the city of Riverview, a hypothetical midwestern city in the US.

`r optbegin('Data: riverview.csv. Click the plus to expand &#8594;', FALSE, show = TRUE, toggle = params$TOGGLE)`
**Download link**

[Download the data here](https://uoepsy.github.io/data/riverview.csv){target="_blank"}

**Description**

The riverview data come from @Lewis-Beck2015 and contain five attributes collected from a random sample of $n=32$ employees working for the city of Riverview, a hypothetical midwestern city in the US. The attributes include:

- `education`: Years of formal education
- `income`: Annual income (in thousands of U.S. dollars)
- `seniority`: Years of seniority
- `gender`: Employee's gender
- `male`: Dummy coded gender variable (0 = Female, 1 = Male)
- `party`: Political party affiliation


**Preview**

The first six rows of the data are:

```{r echo=FALSE}
library(tidyverse)
library(kableExtra)

riverview <- read_csv('https://uoepsy.github.io/data/riverview.csv')
kable(head(riverview), align='c') %>% kable_styling(full_width = FALSE)
```
`r optend()`

<br>



`r qbegin(1)`
Load the required libraries and import the riverview data into a variable named `riverview`.

_**Tip:**
The data file is called 'riverview.csv'. Hence, the data are in comma separated value (csv) format and the appropriate `tidyverse` function is `read_csv()`._

**Steps:**

1. Load the tidyverse library
2. Read the data into R using `read_csv()`
3. Check that everything went correctly by inspecting the top 6 rows of the data with the function `head()`.
`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`
Load the tidyverse library:
```{r, warning=FALSE, message=FALSE}
library(tidyverse)
```

Read the data into R:
```{r}
riverview <- read_csv(file = "https://uoepsy.github.io/data/riverview.csv")
```

Check for reading errors by inspecting the first six rows of the data:
```{r}
head(riverview)
```
`r solend()`



# Data exploration

## Marginal distributions

Typical steps when examining the marginal distribution of a numeric variable are:

1. *Visualise the distribution of the variable*. 
You could use, for example, `geom_density()` for a density plot or `geom_histogram()` for a histogram.

1. *Comment on the shape of the distribution*. 
Look at the shape, centre and spread of the distribution.
Is it symmetric or skewed? Is it unimodal or bimodal?

1. *Identify any unusual observations*. 
Do you notice any extreme observations?

`r qbegin(2)`
Display and describe the marginal distribution of employee incomes.
`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`
We can plot the marginal distribution of employee incomes as a density curve, and add a boxplot underneath to check for the presence of outliers.

```{r fig.cap="Density plot and boxplot of employee incomes."}
ggplot(data = riverview, aes(x = income)) +
  geom_density() +
  geom_boxplot(width = 1/300) +
  labs(x = "Income (in thousands of U.S. dollars)", 
       y = "Probability density")
```

The plot suggests that the distribution of employee incomes is unimodal and most of the incomes are between roughly \$45,000 and \$70,000. 
The smallest income in the sample is about \$25,000 and the largest income is over \$80,000. (We could find the exact values using the `summary()` function).
This suggests there is a fair amount of variation in the data. 
Furthermore, the boxplot does not highlight any outliers in the data.

To further summarize the distribution, it is typical to compute and report numerical summary statistics such as the mean and standard deviation. One way to compute these values is to use the `summary()` function from the `tidyverse` library:

```{r}
riverview %>% 
  summarize(
    M = mean(income), 
    SD = sd(income)
    )
```

Following the exploration above, we can describe this variable as follows:

:::int
The marginal distribution of income is unimodal with a mean of approximately \$53,700. There is variation in employees' salaries (SD = \$14,553). 
:::

`r solend()`




`r qbegin(3)`
Display and describe the marginal distribution of education level.
`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`
We can visualise the marginal distribution of education level using a density curve, and add a boxplot underneath to check for the presence of outliers.

```{r fig.cap='Density plot and boxplot of employee education levels.'}
ggplot(data = riverview, aes(x = education)) +
  geom_density() +
  geom_boxplot(width = 1/100) +
  labs(x = "Education (in years)", 
       y = "Probability density")
```

Below are the summary statistics for the employees' level of education:
```{r}
riverview %>%
  summarize(
    M = mean(education),
    SD = sd(education)
    )
```

Again, we might write:

:::int
The marginal distribution of education is unimodal with a mean of 16 years. There is variation in employees' level of education (SD = 4.4 years).
:::
`r solend()`





## Relationship between variables {-}

After examining the marginal distributions of the variables of interest in the analysis, we typically move on to examining relationships between the variables.

When describing the relationship between two numeric variables, we typically look at their scatterplot and comment on four characteristics of the relationship:

1. The *direction* of the association indicates whether large values of one variable tend to go with large values of the other (positive association) or with small values of the other (negative association).
1. The *form* of association refers to whether the relationship between the variables can be summarized well with a straight line or some more complicated pattern.
1. The *strength* of association entails how closely the points fall to a recognizable pattern such as a line.
1. *Unusual observations* that do not fit the pattern of the rest of the observations and which are worth examining in more detail.

`r qbegin(4)`
Create a scatterplot of income and education level.

Describe the relationship between income and level of education among the employees in the sample.

_**Hint:** To comment on the strength of a linear association, compute the correlation coefficient with the `cor()` function._
`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`
We are trying to investigate how income varies when varying years of formal education.
Hence income is the dependent variable (on the y-axis), and education is the independent variable (on the x-axis).

```{r riverview-scatterplot, fig.cap='The relationship between employees\' education level and income.'}
ggplot(data = riverview, aes(x = education, y = income)) +
  geom_point(alpha = 0.5) +
  labs(x = "Education (in years)", 
       y = "Income (in thousands of U.S. dollars)")
```

To comment on the strength of the linear association we compute the correlation coefficient:
```{r}
riverview %>%
  select(education, income) %>%
  cor()
```

that is, 
$$
r_{Education,\ Income} = 0.79
$$


We might write:

:::int
There is a strong positive linear relationship between education level and income for the employees in the sample.
High incomes tend to be observed, on average, with more years of formal education.
The scatterplot does not highlight any outliers.
:::
`r solend()`



# Model specification and fitting

The scatterplot highlights a linear relationship, where the data points are scattered around an underlying linear pattern with a roughly-constant spread as x varies.

Hence, we will try to fit a simple (= one x variable only) linear regression model:

$$
y = \beta_0 + \beta_1 x + \epsilon \quad \text{where} \quad \epsilon \sim N(0, \sigma) \text{ independently}
$$

where "$\epsilon \sim N(0, \sigma) \text{ independently}$" means that the errors are normally scattered around the line with mean zero and constant spread as x varies.


`r qbegin(5)`
Fit the linear model to the sample data using the `lm()` function and name the output `mdl`.

Write down the equation of the fitted line.

_**Hint:**_
_The syntax of the `lm()` function is: _
```
lm(<response variable> ~ 1 + <explanatory variable>, data = <dataframe>)
```
`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`
The fitted model can be written as
$$
\widehat{Income} = \hat \beta_0 + \hat \beta_1 \ Education
$$
or
$$
\widehat{Income} = \hat \beta_0 \cdot 1 + \hat \beta_1 \cdot Education
$$

When we specify the linear model in R, we include after the tilde sign, `~`, the variables that appear to the right of the $\hat \beta$s. That's why the 1 is included.

As the variables are in the `riverview` dataframe, we would write:
```{r}
mdl <- lm(income ~ 1 + education, data = riverview)
mdl
```

Note that by calling the name of the fitted model, `mdl`, you can see the estimated regression coefficients $\hat \beta_0$ and $\hat \beta_1$. The fitted line is:
$$
\widehat{Income} = 11.32 + 2.65 \ Education \\
$$
`r solend()`


`r qbegin(6)`
Explore the following equivalent ways to obtain the estimated regression coefficients --- that is, $\hat \beta_0$ and $\hat \beta_1$ --- from the fitted model:

- `mdl`
- `mdl$coefficients`
- `coef(mdl)`
- `coefficients(mdl)`
- `summary(mdl)` and look at the "Estimate" column
`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`
To obtain the estimated regression coefficients you can either:

- type `mdl`, i.e. simply invoke the name of the fitted model;
- type `mdl$coefficients`;
- use the `coef(mdl)` function;
- use the `coefficients(mdl)` function;
- use the `summary(mdl)` function and look under the "Estimate" column.

The estimated parameters returned by the above methods are all equivalent. However, `summary()` returns more information and you need to look under the column "Estimate".

```{r}
mdl
mdl$coefficients
coef(mdl)
coefficients(mdl)
summary(mdl)
```

The estimated intercept is $\hat \beta_0 = 11.32$ and the estimated slope is $\hat \beta_1 = 2.65$.
`r solend()`


`r qbegin(7)`
Interpret the estimated intercept and slope in the context of the question of interest.
`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`
We can interpret the estimated intercept as follows,

:::int
The estimated average income associated to zero years of formal education is \$11,321.
:::

For the estimated slope we might write,

:::int
The estimated increase in average income associated to a one year increase in education is \$2,651.
:::
`r solend()`


`r qbegin(8)`
Plot the data and the fitted regression line. To do so:

- Extract the estimated regression coefficients e.g. via `betas <- coef(mdl)`
- Extract the first entry of `betas` via `betas[1]`
- Extract the second entry of `betas` via `betas[2]`
- Provide the intercept and slope to the function
```
geom_abline(intercept = <intercept>, slope = <slope>)
```
`r qend()`


`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`

The function `coef(mdl)` returns a *vector*: that is, a sequence of numbers all of the same type.
To get the first element of the sequence you append `[1]`, and `[2]` for the second.

We can plot the model as follows:
```{r}
betas <- coef(mdl)
intercept <- betas[1]
slope <- betas[2]

ggplot(data = riverview, aes(x = education, y = income)) +
  geom_point(alpha = 0.5) +
  geom_abline(intercept = intercept, slope = slope, 
              color = 'blue', size = 1) + 
  labs(x = "Education (in years)", 
       y = "Income (in thousands of U.S. dollars)")
```
`r solend()`


# Fitted and predicted values

To compute the model-predicted values for the data in the sample:

- `predict(<fitted model>)`
- `fitted(<fitted model>)`
- `fitted.values(<fitted model>)`
- `mdl$fitted.values`

```{r}
predict(mdl)
```


To compute model-predicted values for other data:

- `predict(<fitted model>, newdata = <dataframe>)`

```{r}
education_query <- tibble(education = c(11, 23))
predict(mdl, newdata = education_query)
```



# Residuals

The residuals represent the deviations between the actual responses and the predicted responses and can be obtained either as

- `mdl$residuals`;
- `resid(mdl)`;
- `residuals(mdl)`;
- computing them as the difference between the response and the predicted response.


`r qbegin(9)`
Use `predict(mdl)` to compute the fitted values and residuals. Mutate the `riverview` dataframe to include the fitted values and residuals as extra columns.

Assign to the following symbols the corresponding numerical values:

- $y_{3}$ = response variable for unit $i = 3$ in the sample data
- $\hat y_{3}$ = fitted value for the third unit
- $\hat \epsilon_{5} = y_{5} - \hat y_{5}$ = the residual corresponding to the 5th unit.
`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`
```{r}
riverview_fitted <- riverview %>%
  mutate(
    income_hat = predict(mdl),
    resid = income - income_hat
  )

head(riverview_fitted)
```

- $y_{3}$ = 47.03
- $\hat y_{3}$ = 37.83
- $\hat \epsilon_{5} = y_{5} - \hat y_{5}$ = -12.36

`r solend()`




# Inference for regression coefficients

Consider again the output of the `summary()` function:
```{r}
summary(mdl)
```

To quantify the amount of uncertainty in each estimated coefficient that is due to sampling variability, we use the standard error (SE) of the coefficient. 
_Recall that a standard error gives a numerical answer to the question of how variable a statistic will be because of random sampling._

The standard errors are found in the column "Std. Error". That is, the SE of the intercept is 6.1232, and the SE of the slope corresponding to the education variable is 0.3696.

In this example the slope, 2.651, has a standard error of 0.37. One way to envision this is as a distribution. Our best guess (mean) for the slope parameter is 2.651. The standard deviation of this distribution is 0.37, which indicates the precision (uncertainty) of our estimate.

```{r echo=FALSE, fig.cap='Sampling distribution of the slope coefficient. The distribution is approximately bell-shaped with a mean of 2.651 and a standard error of 0.37.'}
ggplot(tibble(x = c(-3 * 0.37 + 2.651, 3 * 0.37 + 2.651)), aes(x = x)) +
    stat_function(fun = dnorm, args = list(mean = 2.651, sd = 0.37)) +
  labs(x = "Estimate for employee incomes", y = '')
```

It shouldn't surprise you that the reference distribution in this case is a t-distribution with $n-2$ degrees of freedom, where $n$ is the sample size.
Recall the main formulas for obtaining a confidence interval and a test-statistic:

:::frame
**Test statistic**

A test statistic for the null hypothesis $H_0: \beta_1 = 0$ is
$$
t = \frac{\hat \beta_1 - 0}{SE(\hat \beta_1)}
$$
which follows a t-distribution with $n-2$ degrees of freedom.

**Confidence interval**

A confidence interval for the population slope is
$$
\hat \beta_1 \pm t^* \cdot SE(\hat \beta_1)
$$
where $t^*$ denotes the critical value chosen from t-distribution with $n-2$ degrees of freedom for a desired $\alpha$ level of confidence. 
:::


`r qbegin(10)`
Test the hypothesis that the population slope is zero --- that is, that there is no linear association between income and education level in the population.
`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`
We calculate the test statistic
$$
t = \frac{\hat \beta_1 - 0}{SE(\hat \beta_1)} = \frac{ 2.6513 - 0 }{0.3696} = 7.173
$$
and compare it with the 5% critical value from a t-distribution with $n-2$ degrees of freedom, which is:
```{r}
n <- nrow(riverview)
tstar <- qt(0.975, df = n - 2)
tstar
```

As $|t|$ is much larger than $t^*$, we reject then null hypothesis as have strong evidence against it.

The p-value, shown below, also confirms the conclusion.
```{r}
2 * (1 - pt(7.173, n - 2))
```

Please note that the same information was already contained in the row corresponding to the variable "education" in the output of `summary(mdl)`, which reported the t-statistic under `t value` and the p-value under `Pr(>|t|)`:
```{r}
summary(mdl)
```

Before we interpret the results, recall that the p-value `5.56e-08` in the `Pr(>|t|)` column simply means $5.56 \times 10^{-8}$. This is a very small value, hence we will report it as <.001 following the APA guidelines.

:::int
We performed a t-test against the null hypothesis that education is not a significant predictor of income: $t(30) = 7.173,\ p < .001$, two-sided.
The large t-statistic leads to a very small p-value, meaning that we have strong evidence against the null hypothesis.
:::

`r solend()`


`r qbegin(11)`
Compute a confidence interval for the regression slope.
`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`
In the riverview example, for 95\% confidence we have $t^* = 2.04$:
```{r}
n <- nrow(riverview)
tstar <- qt(0.975, df = n - 2)
tstar
```

The confidence interval is:
```{r}
beta1_ci <- tibble(
  lower = 2.6513 - tstar * 0.3696,
  upper = 2.6513 + tstar * 0.3696,
)
beta1_ci
```

In R it is easy to obtain the confidence intervals for the regression coefficients using the command `confint()`:
```{r}
confint(mdl, level = 0.95)
```
The result is exactly the same (up to rounding errors) as the previous one.

We typically report our uncertainty in a statistic by providing $\text{estimate} \pm t^* \cdot \text{SE}$. Here we would say that because of sampling variation, we are 95\% confident that the slope is between 1.896 and 3.406. Interpreting this, we might say,

:::int
For all Riverview city employees, each one-year difference in formal education is associated with a difference in income between \$1,896 and \$3,406, on average.
:::
    
Similarly, we could express the uncertainty in the intercept $\hat \beta_0$ as:

:::int
The average income for all Riverview city employees with zero years of education is between \$-1184 and \$23,827.
:::
`r solend()`



# Partitioning variation

We might ask ourselves if the model is useful. To quantify and assess model utility, we split the total variability of the response into two terms: the variability explained by the model plus the variability left unexplained in the residuals.

$$
\text{total variability in response = variability explained by model + unexplained variability in residuals}
$$

Each term is quantified by a sum of squares:

$$
\begin{aligned}
SS_{Total} &= SS_{Model} + SS_{Residual} \\
\sum_{i=1}^n (y_i - \bar y)^2 &= \sum_{i=1}^n (\hat y_i - \bar y)^2 + \sum_{i=1}^n (y_i - \hat y_i)^2
\end{aligned}
$$


`r qbegin(12)`
What is the proportion of the total variability in incomes explained by the linear relationship with education level?

_**Hint:** The question asks to compute the value of $R^2$._
`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`
The proportion of the total variability in incomes explained by the linear relationship with education level is given by R-squared.

**Option 1**

The R-squared coefficient is defined as:
$$
R^2 = \frac{SS_{Model}}{SS_{Total}} = 1 - \frac{SS_{Residual}}{SS_{Total}}
$$

In R we can write:
```{r}
riverview_fitted <- riverview %>%
  mutate(
    income_hat = predict(mdl),
    resid = income - income_hat
  )
head(riverview_fitted)

riverview_fitted %>%
  summarise(
    SSModel = sum( (income_hat - mean(income))^2 ),
    SSTotal = sum( (income - mean(income))^2 ),
    RSquared = SSModel / SSTotal
  )
```


---

**Option 2**

```{r}
summary(mdl)
```

The output of `summary()` displays the R-squared value in the following line:
```
Multiple R-squared:  0.6317
```

For the moment, ignore "Adjusted R-squared". We will come back to this later in the course.

---

**Option 3**

We can perform an **AN**alysis **O**f **VA**riance or, in short, ANOVA.
It simply means that we are examining/partitioning the total variability of a response variable.

The `anova()` function returns the sum of squares of interest in the column `Sum Sq`:
```{r}
mdl_anova <- anova(mdl)
mdl_anova
```

Consider the column `Sum Sq`. 
The entry corresponding to `education` gives $SS_{Model}$ = 4147.3, as education is the explanatory variable. The entry corresponding to `Residuals` gives $SS_{Residual}$ = 2418.2.

```{r}
# Because the column name Sum Sq has a space, 
# we need to wrap it with backticks
SSModel <- mdl_anova$`Sum Sq`[1]
SSResidual <- mdl_anova$`Sum Sq`[2]
SSTotal <- SSModel + SSResidual

RSquared <- SSModel / SSTotal
RSquared
```

---

**Interpretation**

:::int
Approximately 63\% of the total variability in employee incomes is explained by the linear association with education level.
:::

`r solend()`



# Model utility test

To test if the model is useful --- that is, if the explanatory variable is a useful predictor of the response --- we test the following hypotheses:

$$
\begin{aligned}
H_0 &: \text{the model is ineffective, } \beta_1 = 0 \\
H_1 &: \text{the model is effective, } \beta_1 \neq 0
\end{aligned}
$$


The relevant test-statistic is the F-statistic:

$$
\begin{split}
F = \frac{MS_{Model}}{MS_{Residual}} = \frac{SS_{Model} / 1}{SS_{Residual} / (n-2)}
\end{split}
$$

which compares the amount of variation in the response explained by the model to the amount of variation left unexplained in the residuals.

The sample F-statistic is compared to an F-distribution with $df_{1} = 1$ and $df_{2} = n - 2$ degrees of freedom.^[
$SS_{Total}$ has $n - 1$ degrees of freedom as one degree of freedom is lost in estimating the population mean with the sample mean $\bar{y}$.
$SS_{Residual}$ has $n - 2$ degrees of freedom. There are $n$ residuals, but two degrees of freedom are lost in estimating the intercept and slope of the line used to obtain the $\hat y_i$s.
Hence, by difference, $SS_{Model}$ has $n - 1 - (n - 2) = 1$ degree of freedom.
]


`r qbegin(13)`
Perform a model utility test at the 5\% significance level, by computing the F-statistic using its definition.
`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`
```{r}
df1 <- 1
df2 <- nrow(riverview) - 2
f_star <- qf(0.95, df1, df2)
f_star
```

```{r}
model_utility <- riverview_fitted %>%
  summarise(
    SSModel = sum( (income_hat - mean(income))^2 ),
    SSResid = sum( resid^2 ),
    MSModel = SSModel / 1,
    MSResid = SSResid / df2,
    FObs = MSModel / MSResid
  )
model_utility
```


:::int
We performed an F-test of model utility at the 5\% significance level, where $F(1,30) = 51.45$.

As the observed $F = 51.45$ is much larger than the critical value $F^* = 4.17$, we have strong evidence to reject the null hypothesis that the model is ineffective.
:::

<br>
Alternatively, we can compute the p-value:
```{r}
pvalue <- 1 - pf(model_utility$FObs, df1, df2)
pvalue
```
The value `5.562116e-08` simply means $5.56 \times 10^{-8}$, so it's a really small number.

:::int
We performed an F-test of model utility at the 5\% significance level, where $F(1,30) = 51.45, p<.001$.

The p-value (< .001) is much lower than the specified significance level, meaning that we have very strong evidence against  the null hypothesis.
:::

`r solend()`


`r optbegin('Optional: Another formula for the F-test. Click the plus to expand &#8594;', FALSE, show = TRUE, toggle = params$TOGGLE)`
There is an equivalent formula for the F-statistic of model utility. With some algebra, we can show that the F-test for model utility can be seen as testing if the fraction of variability in the response explained by the model is due to chance alone or not:
$$
F = \frac{R^2 / 1}{(1 - R^2) / (n - 2) } = \frac{R^2 / df_{Model}}{(1 - R^2) / df_{Residual} }
$$

Proof:

$$
\begin{aligned}
F = \frac{SS_{Model} / 1}{SS_{Residual} / (n - 2)} 
= \frac{\frac{SS_{Model}}{SS_{Total}}}{\frac{SS_{Residual}}{SS_{Total}} \cdot \frac{1}{(n - 2)}} 
= \frac{R^2 / 1}{(1 - R^2) / (n - 2)}
\end{aligned}
$$
`r optend()`


`r qbegin(14)`
Look at the output of `summary(mdl)` and `anova(mdl)`.

For each output, identify the relevant information to conduct an F-test against the null hypothesis that the model is ineffective at predicting income using education level.
`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`
```{r}
summary(mdl)
```

The relevant row is the following:
```
F-statistic: 51.45 on 1 and 30 DF,  p-value: 5.562e-08
```

---

Instead, the `anova()` output is:
```{r}
anova(mdl)
```

And the relevant entries are:

- the `Df` column, which contains the degrees of freedom;
- `F value = 51.452`, which is the F-statistic;
- `Pr(>F) = 5.562e-08` = $5.562 \times 10^{-8}$, which is the p-value.


---

We might write up the test results as,

:::int
We performed an F-test for the overall significance of the regression, $F(1, 30) = 51.45, p < .001$.
The large F-statistic leads to a very small p-value ($<.001$), meaning that we have very strong evidence against the null hypothesis that the model is ineffective.

In other words, the data provide strong evidence that education is an effective predictor of income.
:::
`r solend()`


`r qbegin(15)`
Consider the `F value` output of `anova(mdl)` and the `t value` for education returned by `summary(mdl)`

```
F value = 51.452
t value = 7.173
```

Do you notice any relationship between the F-statistic for overall model utility and the t-statistic for $H_0: \beta_1 = 0$?
`r qend()`

`r solbegin(show = params$SHOW_SOLS, toggle = params$TOGGLE)`
**In simple linear regression only**, the F-statistic for overall model significance is equal to the square of the t-statistic for $H_0: \beta_1 = 0$.

You can check that the squared t-statistic is equal, up to rounding error, to the F-statistic:
$$
t^2 = F \\
7.173^2 = 51.452
$$

`r optbegin('Optional: Equivalence of t-test for the slope and model utility F-test in SLR. Click the plus to expand &#8594;', FALSE, show = TRUE, toggle = params$TOGGLE)`

Here we will show the equivalence of the F-test for model effectiveness and t-test for the slope.

Recall the formula of the sum of squares due to the model. We will rewrite it in an equivalent form below:
$$
\begin{aligned}
SS_{Model} &= \sum_i (\hat y_i - \bar y)^2 \\
&= \sum_i (\hat \beta_0 + \hat \beta_1 x_i - \bar y)^2 \\
&= \sum_i (\bar y - \hat \beta_1 \bar x + \hat \beta_1 x_i - \bar y)^2 \\
&= \sum_i (\hat \beta_1 (x_i - \bar x))^2 \\
&= \hat \beta_1^2 \sum_i (x_i - \bar x)^2
\end{aligned}
$$

The F-statistic is given by:
$$
\begin{aligned}
F = \frac{SS_{Model} / 1}{SS_{Residual} / (n - 2)} 
= \frac{\hat \beta_1^2 \sum_i (x_i - \bar x)^2}{\hat \sigma^2} 
= \frac{\hat \beta_1^2 }{\hat \sigma^2 / \sum_i (x_i - \bar x)^2}
\end{aligned}
$$

Now recall the formula of the t-statistic,
$$
t = \frac{\hat \beta_1}{SE(\hat \beta_1)} = \frac{\hat \beta_1}{\hat \sigma / \sqrt{\sum_i (x_i - \bar x)^2}}
$$

It is evident that the latter is obtained as the square root of the former.

`r optend()`

`r solend()`

# References

<div class="tocify-extend-page" data-unique="tocify-extend-page" style="height: 0;"></div>

